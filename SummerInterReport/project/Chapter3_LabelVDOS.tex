\section{labelVDOS : Video Annotation Toolbox}

\paragraph{}
When we look at the existing open source video annotation tools such as ViTBaT \cite{biresaw2016vitbat}, VATIC \cite{vondrick2013efficiently}, ViPER \cite{doermann2000tools} and iVAT\cite{bianco2015interactive}, we find that none of them is suitable due to the complexity and effort in usage, and the high chances of error. It is with this motivation that we create our own tool to label Very Dense Object Sequences (aptly named labelVDOS) with the following novel capabilities:
\begin{itemize}
    % \item The ability to efficiently leverage powerful pre-trained deep learning based object detectors \cite{ren2015faster, liu2016ssd, lin2017focal} to provide bounding box proposals while annotating
    \item The ability to leverage interpolation techniques to only have to correct the bounding box for an object once every few frames, rather than draw one for every frame
    \item The ability to simultaneously keep track of multiple objects without mixing IDs even in dense scenarios
    \item The ability to annotate frame-level phenomenon, making it useful even for tasks such as activity recognition
    \item The ability to work on any platform using just Python
\end{itemize}

The proposed tool, labelVDOS, extends the image annotation tool labelImg\cite{LabelImg} to the particular use-case of creating annotations for object detection as well as tracking in videos. Using an interpolation algorithm, is preferred over an object tracker due to the immense likelihood of ID switches in object dense scenarios which can increase the chances of annotation error. The overall user interface is presented in Figure \ref{labelVDOSLayout} and we now discuss the challenge each component of the tool attempts to solve and its implementation.

\begin{figure}
\centering
{
\includegraphics[width=\linewidth]{SummerInterReport/project/images/labelVDOS_interface.png}
\vspace{-1cm}
\caption{\textbf{The User Interface of Our Proposed Tool}}
\label{labelVDOSLayout}
}
\end{figure}

\subsection{Object Propagation}
In order to ensure that the right-IDs are assigned to the right objects across all frames, we design a Propagation Engine. This Engine ensures that the annotator is able to simultaneously keep track of multiple objects of a wide variety of classes without mixing IDs, even in scenarios that have a high object density. This is accomplished is by ensuring that the bounding boxes that are created on a frame, either by drawing or by accepting proposals, show up on all further frames until the user designates that the object has exited the video.
\paragraph{}The likely location of the bounding box in all future frames is computed by the Propagation Engine using a constant velocity motion model. This allows for users to merely have to correct the positions of bounding boxes in future frames rather than to have to redraw and re-assign their IDs on each frame. 

\subsection{Object Interpolation}
The next challenge that increases the effort of annotators is marking the correct location for all objects in each frame. In order to work around this issue, we design an Interpolation Engine that takes in the bounding boxes of an object in two frames and interpolates all intermediate bounding boxes. We currently use linear interpolation for its simplicity. 
\paragraph{}Suppose that $bbox_{A}$ defines the bounding box in frame A, $bbox_{B}$ in frame B ($A<B$) and let a bounding box be represented as:

\begin{equation}
bbox = [x_{min}, y_{min}, x_{max}, y_{max}]
\end{equation}

Then, for a frame t ($A<t<B$), we can interpolate the bounding boxes using the following equation:

\begin{equation}
bbox_t = \frac{t}{B - A - 1} * (bbox_B - bbox_A) + bbox_A
\end{equation}

In this way, we enable annotators to have to only correct the bounding boxes generated by the Propagation Engine once every few frames, rather than to have to do so on each and every frame.

% \begin{figure*}
% \begin{center}
% \includegraphics[width=0.8\linewidth]{labelVDOSLabelIdSelection.png}
% \end{center}
%   \caption{Annotating a Bounding Box: Selecting from a List of Options. In order to prevent annotators from making errors that arise when typing in labels manually each time, we have them select from a hierarchical list of class labels. This is shown in Figure (A-C). Further, this hierarchy is defined by the Dataset Creators in a simple JSON file which is shipped to all annotators along with the tool. Figure (D) shows this Dataset Creator defined JSON file listing the classes}
% \label{labelVDOSLabelIdSelection}
% \end{figure*}

% \subsection{Object Detector}
% We use the fast and powerful Single Shot MultiBox Detector \cite{liu2016ssd} due to its high speed and reasonable accuracy, allowing it to be run even on CPUs in a reasonable amount of time. Our model is pre-trained on PASCAL VOC, with the option to filter proposals for a relevant set of classes.

% The object detector is used to provide proposals in each frame that do not significantly overlap with ground truth bounding boxes and those from the Propagation and Interpolation Engines. In quantitative terms, we define a metric $IoU_{Proposal}$ for a detection given by:

% \begin{align*}
% IoU_{Proposal} = max(IoU(Proposal, Shape 1), ...,IoU(Proposal, Shape K))
% \end{align*}

% where IoU refers to the ratio of the areas of intersection of two bounding boxes to the area of the union of the two. Only if this metric lies below a tunable threshold $IoU_{ProposalMetric}$, do we display that proposal as a candidate for a new object. 

% Further for those detections for which the metric $IoU_{Proposal}$ lies above a tunable threshold $IoU_{CorrectionMetric}$, we can infer that they refer to the same object as that proposed by the Interpolation or Propagation Engines. Hence, we need to be able to infer which of the two - the detector proposal or the Engine proposal - is more accurate.

% The reason that the detection proposal is not always the more accurate one is that in scenarios with heavy occlusion, the detector merely picks the visible region of the object, while the Engine Proposal which is built from user verified ground truths has a more accurate estimate of the shape of an object. In this way, the Engines complement the Object Detector and this discussion motivates the following metric which helps us decide whether to pick the detection proposal or the engine proposal:

% \begin{equation}
% isOccluded = \frac{Area(Detection)}{Area(Shape)}
% \end{equation}

% In case an object is occluded, then the area of the part that is detected will be much less than the interpolated shape proposed, and thus $isOccluded$ will be small. If, however, the detection has nearly the same area as the shape then it is likely that the detection is more accurate and that will be picked. Hence we have a tunable threshold $isOccludedThreshold$ that will decide whether or not to pick a detection based on the $isOccluded$ metric.

% In this way, the object detector helps provide new proposals as well as complements the Interpolation Engine. Moreover, the use of a deep learning object detector means that the model can be gradually fine-tuned as data is annotated - something that allows for incremental learning as proposed in iVAT.

\subsection{User Interface}

Our tool was designed in Python, focusing primarily on creating a simple interface that would allow users to draw bounding boxes and to have to assign an ID to an object only once. In this way, it would not be difficult for users to keep track of multiple IDs in dense scenarios.
\paragraph{}We keep the UI to be minimalistic and intuitive, in an attempt to minimize the time it takes for the user to create and annotate a bounding box. Moreover, in order to minimize the chances of error involved in marking labels, we allow for the dataset creators to predefine the classes and have the annotators select from a fixed menu of choices. Some of the other key features, apart from the intelligent components like the Propagation and Interpolation Engines include:

\begin{itemize}
  \item The ability to show and hide individual bounding boxes easily, preventing the canvas from becoming especially cluttered
  \item The ability to zoom in and out, which ensures that all relevant objects can be easily selected irrespective of their size
  \item The ability to create multiple bounding boxes without having to click on the draw tool repeatedly
\end{itemize}
\paragraph{}Although our tool has been designed for object tracking in object dense scenarios, it can also be extended to behavior level annotations, as well as handle the frame level annotations required in creating datasets for activity recognition and video captioning.

% Further, the output file is also kept minimalistic, so as to allow the dataset creators to effectively convert the annotations into a format of their choice with minimum effort. Finally, we also include a command to convert the annotations made into a video. This allows both the annotators and the dataset creators to rapidly visualize the entire set annotations for any signs of error.
 